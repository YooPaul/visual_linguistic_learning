# Reviewing Visual-Linguistic Learning Papers 
<!---
[![Open in
Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YooPaul/visual_linguistic_learning/blob/master/.ipynb)<br>
-->

Reviewing recent work in self-supervised visual-linguistic representation learning. I will be adding more references to
more papers along the way and uploading quick implementations/write-ups of core ideas and takeaways.  


## References

[1] Sun, Chen, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. "Videobert: A joint model for video and language representation learning." In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7464-7473. 2019.

[2] [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language](https://scontent-ssn1-1.xx.fbcdn.net/v/t39.8562-6/271974914_483120576492438_4239522333319653600_n.pdf?_nc_cat=107&ccb=1-5&_nc_sid=ae5e01&_nc_ohc=HLSTIdOnYI4AX-HIZcp&_nc_ht=scontent-ssn1-1.xx&oh=00_AT9kvs4RGToDPJEwMz5Yjr8GeM5T0RMDwj49D-wNqjBMjQ&oe=61F00351)

